"""
GPT-OSS Ollama ç‰ˆæœ¬å•Ÿå‹•è…³æœ¬
"""

import os
import sys
import subprocess
import webbrowser
import time
import threading
import requests
from pathlib import Path

# è¨­ç½®æ§åˆ¶å°ç·¨ç¢¼ç‚ºUTF-8
if os.name == 'nt':  # Windows
    os.system('chcp 65001 > nul')

def check_ollama_service():
    """æª¢æŸ¥ Ollama æœå‹™æ˜¯å¦é‹è¡Œ"""
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        return response.status_code == 200
    except:
        return False

def start_ollama_service():
    """å•Ÿå‹• Ollama æœå‹™"""
    print("[+] å•Ÿå‹• Ollama æœå‹™...")
    try:
        subprocess.Popen(
            ["ollama", "serve"], 
            stdout=subprocess.DEVNULL, 
            stderr=subprocess.DEVNULL,
            creationflags=subprocess.CREATE_NEW_PROCESS_GROUP if os.name == 'nt' else 0
        )
        
        # ç­‰å¾…æœå‹™å•Ÿå‹•
        for i in range(10):
            if check_ollama_service():
                print("[OK] Ollama æœå‹™å•Ÿå‹•æˆåŠŸ")
                return True
            print(f"[*] ç­‰å¾… Ollama æœå‹™å•Ÿå‹•... ({i+1}/10)")
            time.sleep(2)
        
        print("[ERROR] Ollama æœå‹™å•Ÿå‹•å¤±æ•—")
        return False
        
    except Exception as e:
        print(f"[ERROR] å•Ÿå‹• Ollama æœå‹™æ™‚å‡ºéŒ¯: {e}")
        return False

def check_model_available():
    """æª¢æŸ¥ GPT-OSS æ¨¡å‹æ˜¯å¦å¯ç”¨"""
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=10)
        if response.status_code == 200:
            models = response.json().get('models', [])
            model_names = [model['name'] for model in models]
            
            if 'gpt-oss:20b' in model_names:
                print("[OK] GPT-OSS:20B æ¨¡å‹å¯ç”¨")
                return True
            else:
                print("âš ï¸ GPT-OSS:20B æ¨¡å‹æœªæ‰¾åˆ°")
                print(f"å¯ç”¨æ¨¡å‹: {model_names}")
                
                if model_names:
                    print("\nğŸ’¡ æ‚¨å¯ä»¥é¸æ“‡å…¶ä»–å¯ç”¨æ¨¡å‹ï¼Œæˆ–è€…åŸ·è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£ GPT-OSS:")
                    print("   ollama pull gpt-oss:20b")
                else:
                    print("\nğŸ’¡ è«‹å…ˆå®‰è£ GPT-OSS æ¨¡å‹:")
                    print("   ollama pull gpt-oss:20b")
                return False
        else:
            print(f"[ERROR] ç„¡æ³•ç²å–æ¨¡å‹åˆ—è¡¨: {response.status_code}")
            return False
            
    except Exception as e:
        print(f"[ERROR] æª¢æŸ¥æ¨¡å‹æ™‚å‡ºéŒ¯: {e}")
        return False

def kill_port_8000():
    """æ¸…ç†8000ç«¯å£"""
    try:
        result = subprocess.run(['netstat', '-ano'], capture_output=True, text=True)
        lines = result.stdout.split('\n')
        
        for line in lines:
            if ':8000' in line and 'LISTENING' in line:
                parts = line.split()
                if len(parts) >= 5:
                    pid = parts[-1]
                    print(f"ç™¼ç¾é€²ç¨‹ {pid} å ç”¨ç«¯å£8000ï¼Œæ­£åœ¨çµæŸ...")
                    subprocess.run(['taskkill', '/F', '/PID', pid], capture_output=True)
                    print(f"é€²ç¨‹ {pid} å·²çµæŸ")
                    time.sleep(1)
                    break
    except Exception as e:
        print(f"æ¸…ç†ç«¯å£æ™‚å‡ºéŒ¯: {e}")

def main():
    print("=" * 60)
    print("GPT-OSS Ollama ç‰ˆæœ¬")
    print("   æ›´å¿«ã€æ›´ç©©å®šçš„ AI å°è©±é«”é©—")
    print("=" * 60)
    
    # æ¸…ç†ç«¯å£
    print("[+] æ­£åœ¨æ¸…ç†ç«¯å£8000...")
    kill_port_8000()
    
    # æª¢æŸ¥ Ollama æ˜¯å¦å®‰è£
    try:
        result = subprocess.run(['ollama', '--version'], capture_output=True, text=True)
        if result.returncode == 0:
            print(f"[OK] Ollama å·²å®‰è£: {result.stdout.strip()}")
        else:
            print("[ERROR] Ollama æœªæ­£ç¢ºå®‰è£")
            return
    except FileNotFoundError:
        print("[ERROR] Ollama æœªå®‰è£ï¼Œè«‹å…ˆå®‰è£ Ollama: https://ollama.ai")
        input("æŒ‰ Enter é€€å‡º...")
        return
    
    # æª¢æŸ¥ Ollama æœå‹™
    if not check_ollama_service():
        print("âš ï¸ Ollama æœå‹™æœªé‹è¡Œï¼Œæ­£åœ¨å•Ÿå‹•...")
        if not start_ollama_service():
            print("[ERROR] ç„¡æ³•å•Ÿå‹• Ollama æœå‹™")
            input("æŒ‰ Enter é€€å‡º...")
            return
    else:
        print("[OK] Ollama æœå‹™æ­£åœ¨é‹è¡Œ")
    
    # æª¢æŸ¥æ¨¡å‹
    if not check_model_available():
        print("\n[ERROR] GPT-OSS æ¨¡å‹ä¸å¯ç”¨")
        choice = input("æ˜¯å¦ç¾åœ¨ä¸‹è¼‰ GPT-OSS:20B æ¨¡å‹ï¼Ÿ (y/n): ")
        if choice.lower() == 'y':
            print("[+] æ­£åœ¨ä¸‹è¼‰ GPT-OSS:20B æ¨¡å‹ï¼ˆé€™å¯èƒ½éœ€è¦ä¸€äº›æ™‚é–“ï¼‰...")
            try:
                subprocess.run(['ollama', 'pull', 'gpt-oss:20b'], check=True)
                print("[OK] æ¨¡å‹ä¸‹è¼‰å®Œæˆ")
            except subprocess.CalledProcessError as e:
                print(f"[ERROR] æ¨¡å‹ä¸‹è¼‰å¤±æ•—: {e}")
                input("æŒ‰ Enter é€€å‡º...")
                return
        else:
            print("å–æ¶ˆå•Ÿå‹•")
            return
    
    # è¨­ç½®å·¥ä½œç›®éŒ„
    project_root = Path(__file__).parent
    os.chdir(project_root)
    
    # æª¢æŸ¥å¿…éœ€æ–‡ä»¶
    required_files = [
        "frontend/index_new.html",
        "backend/app_ollama.py"
    ]
    
    print("\n[+] æ–‡ä»¶æª¢æŸ¥:")
    all_files_exist = True
    for file in required_files:
        if os.path.exists(file):
            print(f"[OK] {file}")
        else:
            print(f"âœ— {file} - ç¼ºå¤±")
            all_files_exist = False
    
    if not all_files_exist:
        print("[ERROR] ç¼ºå°‘å¿…éœ€æ–‡ä»¶")
        input("æŒ‰ Enter é€€å‡º...")
        return
    
    # æª¢æŸ¥ä¾è³´
    print("\n[+] ä¾è³´æª¢æŸ¥:")
    try:
        import fastapi, uvicorn, aiohttp
        print("[OK] FastAPIã€Uvicornã€aiohttp å·²å®‰è£")
    except ImportError as e:
        print(f"âš ï¸ ç¼ºå°‘ä¾è³´: {e}")
        print("æ­£åœ¨å®‰è£...")
        try:
            subprocess.run([
                sys.executable, "-m", "pip", "install", 
                "fastapi", "uvicorn[standard]", "aiohttp"
            ], check=True, capture_output=True)
            print("[OK] ä¾è³´å®‰è£å®Œæˆ")
        except Exception as install_error:
            print(f"[ERROR] ä¾è³´å®‰è£å¤±æ•—: {install_error}")
            input("æŒ‰ Enter é€€å‡º...")
            return
    
    print("\n[+] å•Ÿå‹•ä¿¡æ¯:")
    print("â€¢ ä½¿ç”¨ Ollama API")
    print("â€¢ æ¨¡å‹: gpt-oss:20b")
    print("â€¢ åœ°å€: http://127.0.0.1:8000")
    print("â€¢ å¿«é€Ÿã€ç©©å®šçš„å›æ‡‰")
    print("-" * 60)
    
    # å»¶è¿Ÿæ‰“å¼€æµè§ˆå™¨
    def open_browser():
        time.sleep(3)
        try:
            webbrowser.open('http://127.0.0.1:8000')
            print("[OK] ç€è¦½å™¨å·²è‡ªå‹•æ‰“é–‹")
        except:
            print("âš ï¸ è«‹æ‰‹å‹•æ‰“é–‹ http://127.0.0.1:8000")
    
    browser_thread = threading.Thread(target=open_browser, daemon=True)
    browser_thread.start()
    
    try:
        # å•Ÿå‹• Ollama ç‰ˆæœ¬æœå‹™å™¨
        from backend.app_ollama import app
        import uvicorn
        
        print("[+] æ­£åœ¨å•Ÿå‹• Ollama æœå‹™å™¨...")
        uvicorn.run(
            app,
            host="127.0.0.1",
            port=8000,
            log_level="info",
            access_log=True
        )
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ æœå‹™å™¨å·²åœæ­¢")
    except Exception as e:
        print(f"[ERROR] å•Ÿå‹•å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        input("\næŒ‰ Enter é€€å‡º...")

if __name__ == "__main__":
    main()